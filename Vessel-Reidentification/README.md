![Python 3.6](https://img.shields.io/badge/Python-3.6-green.svg)
![PyTorch 1.6](https://img.shields.io/badge/PyTorch-1.6-blue.svg)
# Viewpoint Oriented Maritime Vessel Re-identification

## Model Architecture
<div align="center">
    <img src="/images/Fig4.jpg" width="800" height="400" alt="reid model"/>
</div>

In vessel re-identification, our target is to extract the identity of a given query image using a set of gallery images (the database). Here, the main challenges are the lack of available data in thermal domain for marine vessels and the change of features for each vessel with different camera viewpoint. To tackle these issues, we used a model robust to the viewpoint, which can generalize well with a small amount of data. As the first step, we mask out the foreground (the vessel) from a given frame. Since thermal images do not contain color features and the intensity distribution of the foreground and the background are similar, conventional algorithms such as GrabCut do not perform well in this task. As a solution, we used an encoder-decoder architecture with residual connections to build the foreground mask of a given frame.

## Visualization Example
We visiualize some examples of vessel images and their
- foreground masks generated by an encoder-decoder model,
- foreground masks generated by deep learning network,
- front, rear and side attention masks.
  
<div align="center">
    <img src="/images/enc_dec_out_com.PNG" width="800" height="400" alt="encoder model output"/>
</div>
a) Results of the encoder-decoder model used in foreground extracting. (b) Masks generated for each side; front, side and rear views using the foreground mask.

## Get Started
### Prerequisites
- Download the dataset [[link]](https://drive.google.com/drive/folders/1ocKS8lRjAa0u1lVKcoW-3WSiHPLy1z2r) </br>

- Download this repo by:
```
$ git clone https://github.com/tsaishien-chen/SPAN.git
$ cd SPAN
```
- We run the code under Python 3.6.9
- The versions of used python packages are listed in `requirements.txt`. You can install all the dependancies by:
```
$ sed -i 's/==/>=/g' requirements.txt
$ pip3 install -r requirements.txt
```
### Train
To train the model from scratch, please run
```
$ python3 main.py --mode train --image_root <Path_to_VeRi>
```
For example,
```
$ python3 main.py --mode train --image_root ../Dataset/VeRi
```
The whole training process includes five steps:
1. generating the foreground masks by grabcut,
2. training network to generate more robust foreground mask,
3. generating the foreground masks by deep learning network,
4. training network to generate part (front, rear and side) attention mask, and
5. generating part attention masks.

### Implement
We have given the pretrained model of part attention generator; </br>
therefore, you can simply generate the part attention mask without training by
```
$ python3 main.py --mode implement --image_root <Path_to_VeRi>
```
For example,
```
$ python3 main.py --mode implement --image_root ../Dataset/VeRi
```

### Visualize
After training and implementation process, the code will automatically visualize generated masks as above. </br>
Or, you can uncomment the `visualize` function in `main.py` and can independently visualize the masks after being generated in each step.

## Contact
[Tsai-Shien Chen](https://tsaishien-chen.github.io/), [Media IC and System Lab](http://media.ee.ntu.edu.tw/), [National Taiwan University](https://www.ntu.edu.tw/english/index.html) </br>
E-mail : tschen@media.ee.ntu.edu.tw
